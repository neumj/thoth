{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mjn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "matplotlib.use('Agg')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocess(doc_string):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    tokens = word_tokenize(doc_string)\n",
    "    remove_punct = [word.lower() for word in tokens if word.isalnum()]\n",
    "    remove_stops = [word for word in remove_punct if word not in stopwords_english]\n",
    "    stemmed = [stemmer.stem(word) for word in remove_stops]\n",
    "\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def vocabulary_dict(tokens_list):\n",
    "    idx = 0\n",
    "    vocab = {}\n",
    "    for word in tokens_list:\n",
    "        if word not in vocab.keys():\n",
    "            vocab.update({word: idx})\n",
    "            idx += 1\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokens_to_vector(tokens, vocabulary):\n",
    "    vector = np.zeros((1,len(vocabulary.keys())))\n",
    "    for word in tokens:\n",
    "        if word in vocabulary.keys():\n",
    "            vector[0, int(vocabulary[word])] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "\n",
    "def df_to_raw_string(research_df):\n",
    "    raw_str = \"\"\n",
    "    for row in research_df.iterrows():\n",
    "        row_tup = (row[1]['CBInsightsDescription'], row[1]['QuidDescription'], row[1]['CrunchbaseDescription'])\n",
    "        raw_str += \" \".join(row_tup)\n",
    "        \n",
    "    return raw_str\n",
    "\n",
    "\n",
    "def df_to_array(research_df, vocabulary):\n",
    "    m = research_df.shape[0]\n",
    "    n = len(vocabulary.keys())\n",
    "    data_array = np.zeros((m, n))\n",
    "    idx = 0\n",
    "    for row in research_df.iterrows():\n",
    "        raw_str = \"\"\n",
    "        row_tup = (row[1]['CBInsightsDescription'], row[1]['QuidDescription'], row[1]['CrunchbaseDescription'])\n",
    "        raw_str += \" \".join(row_tup)\n",
    "        tokens = nlp_preprocess(raw_str)\n",
    "        vect = tokens_to_vector(tokens, vocabulary)\n",
    "        data_array[idx] = vect\n",
    "        idx += 1\n",
    "\n",
    "    return data_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/ Test examples: 1479\n"
     ]
    }
   ],
   "source": [
    "# load data, asign classes, concatenate\n",
    "\n",
    "addit_df = pd.read_csv('../../data/additive-manufacturing-advanced-materials.csv')\n",
    "addit_df['class'] = \"robotics-vehicles-defense\"\n",
    "agric_df = pd.read_csv('../../data/agricultural-technology.csv')\n",
    "agric_df['class'] = \"agricultural-technology\"\n",
    "artif_df = pd.read_csv('../../data/artificial-intelligence-machine-learning.csv')\n",
    "artif_df['class'] = \"data-ai-ml\"\n",
    "augme_df = pd.read_csv('../../data/augmented-reality-mixed-reality.csv')\n",
    "augme_df['class'] = \"cyber\"\n",
    "auton_df = pd.read_csv('../../data/autonomous-systems.csv')\n",
    "auton_df['class'] = \"robotics-vehicles-defense\"\n",
    "biote_df = pd.read_csv('../../data/biotechnology-pharmaceuticals.csv')\n",
    "biote_df['class'] = \"biotechnology-pharmaceuticals\"\n",
    "consu_df = pd.read_csv('../../data/consulting-market-research.csv')\n",
    "consu_df['class'] = \"consulting-marketing\"\n",
    "consm_df = pd.read_csv('../../data/consumer-products-services.csv')\n",
    "consm_df['class'] = \"consulting-marketing\"\n",
    "cyber_df = pd.read_csv('../../data/cybersecurity.csv')\n",
    "cyber_df['class'] = \"cyber\"\n",
    "analy_df = pd.read_csv('../../data/data-analytics-it-software.csv')\n",
    "analy_df['class'] = \"data-ai-ml\"\n",
    "educa_df = pd.read_csv('../../data/education-training-professional-development.csv')\n",
    "educa_df['class'] = \"consulting-marketing\"\n",
    "elect_df = pd.read_csv('../../data/electronics-microelectronics-hardware.csv')\n",
    "elect_df['class'] = \"electronics-microelectronics\"\n",
    "energ_df = pd.read_csv('../../data/energy-power.csv')\n",
    "energ_df['class'] = \"electronics-microelectronics\"\n",
    "healt_df = pd.read_csv('../../data/healthcare-human-systems.csv')\n",
    "healt_df['class'] = \"biotechnology-pharmaceuticals\"\n",
    "indus_df = pd.read_csv('../../data/industrial-manufacturing.csv')\n",
    "indus_df['class'] = \"manufacturing-logistics\"\n",
    "intel_df = pd.read_csv('../../data/intelligence-surveillance.csv')\n",
    "intel_df['class'] = \"robotics-vehicles-defense\"\n",
    "logis_df = pd.read_csv('../../data/logistics-distribution.csv')\n",
    "logis_df['class'] = \"manufacturing-logistics\"\n",
    "marke_df = pd.read_csv('../../data/marketing-media.csv')\n",
    "marke_df['class'] = \"consulting-marketing\"\n",
    "model_df = pd.read_csv('../../data/modeling-simulation.csv')\n",
    "model_df['class'] = \"data-ai-ml\"\n",
    "posit_df = pd.read_csv('../../data/position-navigation.csv')\n",
    "posit_df['class'] = \"robotics-vehicles-defense\"\n",
    "robot_df = pd.read_csv('../../data/robotics-mechatronics.csv')\n",
    "robot_df['class'] = \"robotics-vehicles-defense\"\n",
    "space_df = pd.read_csv('../../data/space-aerospace-technology.csv')\n",
    "space_df['class'] = \"robotics-vehicles-defense\"\n",
    "telec_df = pd.read_csv('../../data/telecommunication-systems-services.csv')\n",
    "telec_df['class'] = \"cyber\"\n",
    "vehic_df = pd.read_csv('../../data/vehicle-systems.csv')\n",
    "vehic_df['class'] = \"robotics-vehicles-defense\"\n",
    "\n",
    "#all\n",
    "corpus_df = pd.concat([addit_df, agric_df, artif_df, augme_df, auton_df, \n",
    "                       biote_df, consu_df, consm_df, cyber_df, analy_df, \n",
    "                       educa_df, elect_df, energ_df, healt_df, indus_df,\n",
    "                       intel_df, logis_df, marke_df, model_df, posit_df,\n",
    "                       robot_df, space_df, telec_df, vehic_df], \n",
    "                      axis=0, ignore_index=True)\n",
    "#health, robotics\n",
    "#corpus_df = pd.concat([health_df, robotics_df], \n",
    "#                      axis=0, ignore_index=True)\n",
    "\n",
    "#ai, modeling\n",
    "#corpus_df = pd.concat([aiml_df, modeling_df], \n",
    "#                      axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Train/ Test examples: \" + str(corpus_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 141530\n",
      "Unique tokens: 9605\n"
     ]
    }
   ],
   "source": [
    "# process text\n",
    "raw_str = df_to_raw_string(corpus_df)\n",
    "tokens = nlp_preprocess(raw_str)\n",
    "vocab = vocabulary_dict(tokens)\n",
    "print(\"Total tokens: \" + str(len(tokens)))\n",
    "print(\"Unique tokens: \" + str(len(vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data arrays\n",
    "X = df_to_array(corpus_df, vocab)\n",
    "Y = corpus_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agricultural-technology\n",
      "biotechnology-pharmaceuticals\n",
      "consulting-marketing\n",
      "cyber\n",
      "data-ai-ml\n",
      "electronics-microelectronics\n",
      "manufacturing-logistics\n",
      "robotics-vehicles-defense\n"
     ]
    }
   ],
   "source": [
    "# encode Y labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "a = [print(label) for label in label_encoder.classes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1257\n",
      "Testing examples: 222\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "test_size = 0.15\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size)\n",
    "print(\"Training examples: \" + str(len(y_train)))\n",
    "print(\"Testing examples: \" + str(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=100,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model to training data\n",
    "model = xgb.XGBClassifier(objective=\"multi:softmax\", learning_rate=0.1, max_depth=100, n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.92%\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Front Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary matches: 55.0\n",
      "Class: robotics-vehicles-defense\n"
     ]
    }
   ],
   "source": [
    "pred_str = \"Second Front Systems is a public benefit, venture-backed software company that \\\n",
    "equips defense and national security professionals for long-term, continuous competition \\\n",
    "for access to emerging technologies. To accelerate the delivery of emerging technologies \\\n",
    "to U.S. and Allied warfighters.  To enable enduring strategic advantage for the U.S. and \\\n",
    "its allies through agile, responsive acquisition warfare. While serving in the U.S. Marine \\\n",
    "Corps, our co-founders Peter Dixon and Mark Butler saw firsthand the damages done by an \\\n",
    "outdated acquisition system to those on the frontlines of defending our country. To address \\\n",
    "this critical issue, they formed Second Front Systems as a public benefit corporation with \\\n",
    "the mission of accelerating the transition of technology to U.S. and Allied warfighters. \\\n",
    "Our team is comprised of trailblazers dedicated to bridging the gap between the government and private sector.\"\n",
    "tokens = nlp_preprocess(pred_str)\n",
    "vect = tokens_to_vector(tokens, vocab)\n",
    "print(\"Vocabulary matches: \" + str(vect.sum()))\n",
    "pred = model.predict(vect)\n",
    "pred_class = label_encoder.inverse_transform(pred)\n",
    "print('Class: ' + pred_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary matches: 82.0\n",
      "Class: data-ai-ml\n"
     ]
    }
   ],
   "source": [
    "pred_str = \"Primer enables organizations to quickly explore and utilize the world’s exponentially \\\n",
    "growing sources of text-based information. Our best-in-class natural language processing (NLP) \\\n",
    "engines and applications help you make sense of it all in real-time with human-level precision. \\\n",
    "Request a demo to learn more about: Primer Engines — assemble data processing pipelines with flexible \\\n",
    "building blocks pretrained on domain-specific data. Primer Automate — build your own NLP models, or \\\n",
    "retrain Primer Engines on your own data, with no technical skills required. Primer Analyze — create a \\\n",
    "scalable, self-curating knowledge base that can sift through billions of documents in seconds. \\\n",
    "Primer Extract — explore large caches of data quickly with translation, OCR, and image recognition capabilities. \\\n",
    "UNLOCK MACHINE INTELLIGENCE. Primer provides industrial-grade NLP applications for government agencies, \\\n",
    "financial institutions, Fortune 50 companies, and many other organizations. Organizations collect massive \\\n",
    "amounts of data — far more than human analysts can handle. As a result, much of it remains unexplored or \\\n",
    "underutilized. At Primer, we are dedicated to helping organizations make the best use of their investment in \\\n",
    "data. We do this by using best-in-class machine learning and natural language processing technologies to help \\\n",
    "our customers scale and optimize their intelligence workflows.\"\n",
    "tokens = nlp_preprocess(pred_str)\n",
    "vect = tokens_to_vector(tokens, vocab)\n",
    "print(\"Vocabulary matches: \" + str(vect.sum()))\n",
    "pred = model.predict(vect)\n",
    "pred_class = label_encoder.inverse_transform(pred)\n",
    "print('Class: ' + pred_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anduril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary matches: 106.0\n",
      "Class: robotics-vehicles-defense\n"
     ]
    }
   ],
   "source": [
    "pred_str =\"Anduril builds cutting-edge hardware and software products that solve complex national \\\n",
    "security challenges for America and its allies. At the core of all our products is Lattice, an AI \\\n",
    "software backbone that uses sensor fusion, machine learning, and mesh networking to integrate \\\n",
    "real-time data from Anduril hardware and third-party systems into a single, autonomous operating picture. \\\n",
    "Software Lattice Platform Artificial intelligence At the core of all our products is Lattice, an AI \\\n",
    "software backbone that uses sensor fusion, machine learning, and mesh networking to integrate real-time \\\n",
    "data from Anduril hardware and third-party systems into a single, autonomous operating picture. Hardware \\\n",
    "Sentry Tower Autonomous awareness Sentry towers are equipped with the latest sensors and work together \\\n",
    "via Lattice to augment awareness in remote locations. With a compact footprint, hardened onboard processing \\\n",
    "and solar power architecture, Sentry can be rapidly deployed and sustained with minimal maintenance in \\\n",
    "austere conditions. Hardware Ghost 4 sUAS Intelligent air support Ghost 4 is an autonomous VTOL sUAS that \\\n",
    "operates on the Lattice AI platform. Ghost 4 is modular, man-portable, waterproof, and combines long \\\n",
    "endurance, high payload capacity and a near-silent acoustic signature for a wide variety of mission \\\n",
    "capabilities. Hardware Anvil sUAS Precision kinetic intercept Anvil is the kinetic element of our \\\n",
    "end-to-end cUAS capability. It uses physical speed and onboard guidance to seek and destroy drone \\\n",
    "threats with positive identification and minimal collateral damage. Mission Effective Anduril is a \\\n",
    "product company developing technology that works from day one. We deploy in hours, not years. Our \\\n",
    "partners start receiving actionable intelligence within minutes of activation.\"\n",
    "tokens = nlp_preprocess(pred_str)\n",
    "vect = tokens_to_vector(tokens, vocab)\n",
    "print(\"Vocabulary matches: \" + str(vect.sum()))\n",
    "pred = model.predict(vect)\n",
    "pred_class = label_encoder.inverse_transform(pred)\n",
    "print('Class: ' + pred_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teton Telecom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary matches: 36.0\n",
      "Class: consulting-marketing\n"
     ]
    }
   ],
   "source": [
    "pred_str =\"Who Is Teton Telecom? What We Offer  Mergers and Acquisitions Fractional Executive Service Mergers \\\n",
    "and Acquisitions Fiber Optics Fractional Executive Service Mergers and Acquisitions Fractional Executive \\\n",
    "Service Fractional Executive Service Fractional Executive Service Comprehensive Business Services Comprehensive \\\n",
    "Business Services Comprehensive Business Services Stakeholder Communications and Investor Relations \\\n",
    "Comprehensive Business Services Comprehensive Business Services Underwriting Adding Value Comprehensive \\\n",
    "Business Services Adding Value Social Contact Us Contact Us Help us help you! Let us know the details on \\\n",
    "your project, or whatever it is you're hoping to accomplish. We'll let you know if it's something we can \\\n",
    "help with. We look forward to hearing from you! Teton Telecom Get in Touch  apply. GoDadd\"\n",
    "tokens = nlp_preprocess(pred_str)\n",
    "vect = tokens_to_vector(tokens, vocab)\n",
    "print(\"Vocabulary matches: \" + str(vect.sum()))\n",
    "pred = model.predict(vect)\n",
    "pred_class = label_encoder.inverse_transform(pred)\n",
    "print('Class: ' + pred_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
