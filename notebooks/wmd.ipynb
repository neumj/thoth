{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    \"\"\"\n",
    "    Read json file.\n",
    "    Arguments:\n",
    "     file_path -- string, path to file.\n",
    "    Returns:\n",
    "     d -- dictionary, with json contents.\n",
    "    Tips:\n",
    "     None.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path) as json_data:\n",
    "        d = json.load(json_data)\n",
    "\n",
    "    return d\n",
    "\n",
    "def write_json(data_dict, file_path):\n",
    "    \"\"\"\n",
    "    Write dictionary to json.\n",
    "    Arguments:\n",
    "     data_dict -- dictionary.\n",
    "     file_path -- string, path to file.\n",
    "    Returns:\n",
    "     None.\n",
    "    Tips:\n",
    "     None.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"w\") as fp:\n",
    "        json.dump(data_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vocab(vectorizer, doc_string, quantile):\n",
    "    tfidf_vect = vectorizer.fit_transform(doc_string)\n",
    "    tfidf_array = tfidf_vect.toarray()\n",
    "    tfidf_features = vectorizer.get_feature_names()\n",
    "    hits = np.where(tfidf_array > (tfidf_array.max() * (1 - quantile)))\n",
    "    vocab = []\n",
    "    for idx in hits[1]:\n",
    "        word = tfidf_features[idx]\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "            \n",
    "    return vocab, tfidf_array, tfidf_features\n",
    "\n",
    "def vocab_dict(combined_vocabs):\n",
    "    idx = 0\n",
    "    vocab = {}\n",
    "    for word in combined_vocabs:\n",
    "        if word not in vocab.keys():\n",
    "            vocab.update({word: idx})\n",
    "            idx += 1\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "def doc_to_vector(document, vocabulary):\n",
    "    vector = np.zeros((1,len(vocabulary.keys())))\n",
    "    for word in document.split(' '):\n",
    "        if word in vocabulary.keys():\n",
    "            vector[0, int(vocabulary[word])] = 1\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_docs = read_json(\"../data/positive_docs.json\")\n",
    "negative_docs = read_json(\"../data/negative_docs.json\")\n",
    "nvidia_docs = read_json(\"../data/nvidia_docs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "tech_vocab, tech_tf, tech_feats = tf_vocab(vectorizer, [positive_docs['nlp_string']], 0.85)\n",
    "print(len(tech_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centuri',\n",
       " 'develop',\n",
       " 'field',\n",
       " 'gener',\n",
       " 'human',\n",
       " 'includ',\n",
       " 'mani',\n",
       " 'mathemat',\n",
       " 'natur',\n",
       " 'physic',\n",
       " 'research',\n",
       " 'scienc',\n",
       " 'scientif',\n",
       " 'social',\n",
       " 'studi',\n",
       " 'technolog',\n",
       " 'theori',\n",
       " 'use']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "soc_vocab, soc_tf, soc_feats = tf_vocab(vectorizer, [negative_docs['nlp_string']], 0.85)\n",
    "print(len(soc_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'belief',\n",
       " 'concept',\n",
       " 'cultur',\n",
       " 'develop',\n",
       " 'differ',\n",
       " 'exampl',\n",
       " 'form',\n",
       " 'group',\n",
       " 'human',\n",
       " 'includ',\n",
       " 'peopl',\n",
       " 'philosoph',\n",
       " 'philosophi',\n",
       " 'religi',\n",
       " 'religion',\n",
       " 'social',\n",
       " 'societi',\n",
       " 'studi',\n",
       " 'term',\n",
       " 'theori',\n",
       " 'thought',\n",
       " 'tradit',\n",
       " 'use']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_vocab, nvi_tf, nvi_feats = tf_vocab(vectorizer, [nvidia_docs['nlp_string']], 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['announc',\n",
       " 'compani',\n",
       " 'comput',\n",
       " 'driver',\n",
       " 'gpu',\n",
       " 'graphic',\n",
       " 'hardwar',\n",
       " 'nvidia']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.072546479593212"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = embeddings.wmdistance(tech_vocab, nvi_vocab)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2755192793140884"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = embeddings.wmdistance(soc_vocab, nvi_vocab)\n",
    "distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
