{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mjn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "matplotlib.use('Agg')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocess(doc_string):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    tokens = word_tokenize(doc_string)\n",
    "    remove_punct = [word.lower() for word in tokens if word.isalnum()]\n",
    "    remove_stops = [word for word in remove_punct if word not in stopwords_english]\n",
    "    stemmed = [stemmer.stem(word) for word in remove_stops]\n",
    "\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def vocabulary_dict(tokens_list):\n",
    "    idx = 0\n",
    "    vocab = {}\n",
    "    for word in tokens_list:\n",
    "        if word not in vocab.keys():\n",
    "            vocab.update({word: idx})\n",
    "            idx += 1\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokens_to_vector(tokens, vocabulary):\n",
    "    vector = np.zeros((1,len(vocabulary.keys())))\n",
    "    for word in tokens:\n",
    "        if word in vocabulary.keys():\n",
    "            vector[0, int(vocabulary[word])] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "\n",
    "def df_to_raw_string(research_df):\n",
    "    raw_str = \"\"\n",
    "    for row in research_df.iterrows():\n",
    "        row_tup = (row[1]['CBInsightsDescription'], row[1]['QuidDescription'], row[1]['CrunchbaseDescription'])\n",
    "        raw_str += \" \".join(row_tup)\n",
    "        \n",
    "    return raw_str\n",
    "\n",
    "\n",
    "def df_to_array(research_df, vocabulary):\n",
    "    m = research_df.shape[0]\n",
    "    n = len(vocabulary.keys())\n",
    "    data_array = np.zeros((m, n))\n",
    "    idx = 0\n",
    "    for row in research_df.iterrows():\n",
    "        raw_str = \"\"\n",
    "        row_tup = (row[1]['CBInsightsDescription'], row[1]['QuidDescription'], row[1]['CrunchbaseDescription'])\n",
    "        raw_str += \" \".join(row_tup)\n",
    "        tokens = nlp_preprocess(raw_str)\n",
    "        vect = tokens_to_vector(tokens, vocabulary)\n",
    "        data_array[idx] = vect\n",
    "        idx += 1\n",
    "\n",
    "    return data_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/ Test examples: 355\n"
     ]
    }
   ],
   "source": [
    "health_df = pd.read_csv('../data/healthcare-human-systems.csv')\n",
    "health_df['class'] = 0\n",
    "media_df = pd.read_csv('../data/marketing-media.csv')\n",
    "media_df['class'] = 0\n",
    "education_df = pd.read_csv('../data/education.csv')\n",
    "education_df['class'] = 0\n",
    "\n",
    "aiml_df = pd.read_csv('../data/ai-ml.csv')\n",
    "aiml_df['class'] = 1\n",
    "modeling_df = pd.read_csv('../data/modeling.csv')\n",
    "modeling_df['class'] = 1\n",
    "robotics_df = pd.read_csv('../data/robotics.csv')\n",
    "robotics_df['class'] = 1\n",
    "\n",
    "#all\n",
    "corpus_df = pd.concat([health_df, media_df, education_df, aiml_df, modeling_df, robotics_df], \n",
    "                      axis=0, ignore_index=True)\n",
    "#health, robotics\n",
    "#corpus_df = pd.concat([health_df, robotics_df], \n",
    "#                      axis=0, ignore_index=True)\n",
    "\n",
    "#health, robotics\n",
    "#corpus_df = pd.concat([aiml_df, modeling_df], \n",
    "#                      axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Train/ Test examples: \" + str(corpus_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 34158\n",
      "Unique tokens: 4094\n"
     ]
    }
   ],
   "source": [
    "raw_str = df_to_raw_string(corpus_df)\n",
    "tokens = nlp_preprocess(raw_str)\n",
    "vocab = vocabulary_dict(tokens)\n",
    "print(\"Total tokens: \" + str(len(tokens)))\n",
    "print(\"Unique tokens: \" + str(len(vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_to_array(corpus_df, vocab)\n",
    "Y = corpus_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 284\n",
      "Testing examples: 71\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "test_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "print(\"Training examples: \" + str(len(y_train)))\n",
    "print(\"Testing examples: \" + str(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=100,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model to training data\n",
    "model = XGBClassifier(learning_rate=0.1, max_depth=100, n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.92%\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary matches: 46.0\n",
      "Class: [0]\n"
     ]
    }
   ],
   "source": [
    "pred_str = \"Second Front Systems is a public benefit, venture-backed software company that \\\n",
    "equips defense and national security professionals for long-term, continuous competition \\\n",
    "for access to emerging technologies. To accelerate the delivery of emerging technologies \\\n",
    "to U.S. and Allied warfighters.  To enable enduring strategic advantage for the U.S. and \\\n",
    "its allies through agile, responsive acquisition warfare. While serving in the U.S. Marine \\\n",
    "Corps, our co-founders Peter Dixon and Mark Butler saw firsthand the damages done by an \\\n",
    "outdated acquisition system to those on the frontlines of defending our country. To address \\\n",
    "this critical issue, they formed Second Front Systems as a public benefit corporation with \\\n",
    "the mission of accelerating the transition of technology to U.S. and Allied warfighters. \\\n",
    "Our team is comprised of trailblazers dedicated to bridging the gap between the government and private sector.\"\n",
    "tokens = nlp_preprocess(pred_str)\n",
    "vect = tokens_to_vector(tokens, vocab)\n",
    "print(\"Vocabulary matches: \" + str(vect.sum()))\n",
    "pred = model.predict(vect)\n",
    "print('Class: ' + str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary matches: 79.0\n",
      "Class: [1]\n"
     ]
    }
   ],
   "source": [
    "pred_str = \"Primer enables organizations to quickly explore and utilize the worldâ€™s exponentially \\\n",
    "growing sources of text-based information. Our best-in-class natural language processing (NLP) \\\n",
    "engines and applications help you make sense of it all in real-time with human-level precision. \\\n",
    "Request a demo to learn more about: Primer Engines â€” assemble data processing pipelines with flexible \\\n",
    "building blocks pretrained on domain-specific data. Primer Automate â€” build your own NLP models, or \\\n",
    "retrain Primer Engines on your own data, with no technical skills required. Primer Analyze â€” create a \\\n",
    "scalable, self-curating knowledge base that can sift through billions of documents in seconds. \\\n",
    "Primer Extract â€” explore large caches of data quickly with translation, OCR, and image recognition capabilities. \\\n",
    "UNLOCK MACHINE INTELLIGENCE. Primer provides industrial-grade NLP applications for government agencies, \\\n",
    "financial institutions, Fortune 50 companies, and many other organizations. Organizations collect massive \\\n",
    "amounts of data â€” far more than human analysts can handle. As a result, much of it remains unexplored or \\\n",
    "underutilized. At Primer, we are dedicated to helping organizations make the best use of their investment in \\\n",
    "data. We do this by using best-in-class machine learning and natural language processing technologies to help \\\n",
    "our customers scale and optimize their intelligence workflows.\"\n",
    "tokens = nlp_preprocess(pred_str)\n",
    "vect = tokens_to_vector(tokens, vocab)\n",
    "print(\"Vocabulary matches: \" + str(vect.sum()))\n",
    "pred = model.predict(vect)\n",
    "print('Class: ' + str(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
