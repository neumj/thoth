{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    \"\"\"\n",
    "    Read json file.\n",
    "    Arguments:\n",
    "     file_path -- string, path to file.\n",
    "    Returns:\n",
    "     d -- dictionary, with json contents.\n",
    "    Tips:\n",
    "     None.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path) as json_data:\n",
    "        d = json.load(json_data)\n",
    "\n",
    "    return d\n",
    "\n",
    "def write_json(data_dict, file_path):\n",
    "    \"\"\"\n",
    "    Write dictionary to json.\n",
    "    Arguments:\n",
    "     data_dict -- dictionary.\n",
    "     file_path -- string, path to file.\n",
    "    Returns:\n",
    "     None.\n",
    "    Tips:\n",
    "     None.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"w\") as fp:\n",
    "        json.dump(data_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vocab(vectorizer, documents_list, quantile):\n",
    "    tfidf_vect = vectorizer.fit_transform(documents_list)\n",
    "    tfidf_array = tfidf_vect.toarray()\n",
    "    tfidf_features = vectorizer.get_feature_names()\n",
    "    hits = np.where(tfidf_array > (tfidf_array.max() * (1 - quantile)))\n",
    "    vocab = []\n",
    "    for idx in hits[1]:\n",
    "        word = tfidf_features[idx]\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "            \n",
    "    return vocab, tfidf_array, tfidf_features\n",
    "\n",
    "def vocab_dict(combined_vocabs):\n",
    "    idx = 0\n",
    "    vocab = {}\n",
    "    for word in combined_vocabs:\n",
    "        if word not in vocab.keys():\n",
    "            vocab.update({word: idx})\n",
    "            idx += 1\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "def doc_to_vector(document, vocabulary):\n",
    "    vector = np.zeros((1,len(vocabulary.keys())))\n",
    "    for word in document.split(' '):\n",
    "        if word in vocabulary.keys():\n",
    "            vector[0, int(vocabulary[word])] = 1\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-529292fc1243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/positive_docs.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/negative_docs.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnvidia_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/nvidia_docs.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_json' is not defined"
     ]
    }
   ],
   "source": [
    "positive_docs = read_json(\"../data/positive_docs.json\")\n",
    "negative_docs = read_json(\"../data/negative_docs.json\")\n",
    "nvidia_docs = read_json(\"../data/nvidia_docs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [positive_docs[k] for k in positive_docs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = [negative_docs[k] for k in negative_docs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi = [nvidia_docs[k] for k in nvidia_docs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_vocab, tech_tf, tech_feats = tf_vocab(vectorizer, pos, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human',\n",
       " 'humans',\n",
       " 'new',\n",
       " 'science',\n",
       " 'technological',\n",
       " 'technology',\n",
       " 'tools',\n",
       " 'use',\n",
       " 'used',\n",
       " 'applied',\n",
       " 'research',\n",
       " 'analysis',\n",
       " 'anthropology',\n",
       " 'communication',\n",
       " 'disciplines',\n",
       " 'economics',\n",
       " 'field',\n",
       " 'fields',\n",
       " 'geography',\n",
       " 'history',\n",
       " 'law',\n",
       " 'methods',\n",
       " 'natural',\n",
       " 'political',\n",
       " 'psychology',\n",
       " 'sciences',\n",
       " 'social',\n",
       " 'sociology',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'theory',\n",
       " 'century',\n",
       " 'development',\n",
       " 'formal',\n",
       " 'greek',\n",
       " 'knowledge',\n",
       " 'mathematics',\n",
       " 'medicine',\n",
       " 'modern',\n",
       " 'nature',\n",
       " 'philosophy',\n",
       " 'physics',\n",
       " 'public',\n",
       " 'scientific',\n",
       " 'scientists',\n",
       " 'world',\n",
       " 'aristotle',\n",
       " 'astronomy',\n",
       " 'earth',\n",
       " 'including',\n",
       " 'animals',\n",
       " 'life',\n",
       " 'million',\n",
       " 'ocean',\n",
       " 'species',\n",
       " 'surface',\n",
       " 'water',\n",
       " 'years',\n",
       " 'geometry',\n",
       " 'mathematical',\n",
       " 'mathematicians',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'problems']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_vocab, soc_tf, soc_feats = tf_vocab(vectorizer, neg, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food',\n",
       " 'social',\n",
       " 'societies',\n",
       " 'society',\n",
       " 'cultural',\n",
       " 'culture',\n",
       " 'human',\n",
       " 'studies',\n",
       " 'art',\n",
       " 'arts',\n",
       " 'music',\n",
       " 'century',\n",
       " 'developed',\n",
       " 'knowledge',\n",
       " 'philosophical',\n",
       " 'philosophy',\n",
       " 'schools',\n",
       " 'thought',\n",
       " 'traditions',\n",
       " 'mind',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'different',\n",
       " 'example',\n",
       " 'like',\n",
       " 'mental',\n",
       " 'people',\n",
       " 'religion',\n",
       " 'religious',\n",
       " 'sense',\n",
       " 'terms',\n",
       " 'true']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_vocab, nvi_tf, nvi_feats = tf_vocab(vectorizer, nvi, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['announced',\n",
       " 'based',\n",
       " 'company',\n",
       " 'geforce',\n",
       " 'gpu',\n",
       " 'gpus',\n",
       " 'graphics',\n",
       " 'hardware',\n",
       " 'nvidia']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_str = \" \".join(tech_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_str = \" \".join(soc_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_str = \" \".join(nvi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'announced based company geforce gpu gpus graphics hardware nvidia'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvi_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_vocab = tech_vocab + soc_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_dict(aggregate_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['human', 'humans', 'new', 'science', 'technological', 'technology', 'tools', 'use', 'used', 'applied', 'research', 'analysis', 'anthropology', 'communication', 'disciplines', 'economics', 'field', 'fields', 'geography', 'history', 'law', 'methods', 'natural', 'political', 'psychology', 'sciences', 'social', 'sociology', 'studies', 'study', 'theory', 'century', 'development', 'formal', 'greek', 'knowledge', 'mathematics', 'medicine', 'modern', 'nature', 'philosophy', 'physics', 'public', 'scientific', 'scientists', 'world', 'aristotle', 'astronomy', 'earth', 'including', 'animals', 'life', 'million', 'ocean', 'species', 'surface', 'water', 'years', 'geometry', 'mathematical', 'mathematicians', 'number', 'numbers', 'problems', 'food', 'societies', 'society', 'cultural', 'culture', 'art', 'arts', 'music', 'developed', 'philosophical', 'schools', 'thought', 'traditions', 'mind', 'belief', 'beliefs', 'believe', 'different', 'example', 'like', 'mental', 'people', 'religion', 'religious', 'sense', 'terms', 'true'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "33\n",
      "91\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(tech_vocab))\n",
    "print(len(soc_vocab))\n",
    "print(len(vocab))\n",
    "print(len(tech_vocab) - len(soc_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_vect = doc_to_vector(tech_str, vocab)\n",
    "np.sum(tech_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_vect = doc_to_vector(soc_str, vocab)\n",
    "np.sum(soc_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvi_vect = doc_to_vector(nvi_str, vocab)\n",
    "np.sum(nvi_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(nvi_vect, Y=soc_vect, metric='correlation')\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(nvi_vect, Y=tech_vect, metric='correlation')\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvi_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_headlines = ['i like cats', 'one two three', 'i like dogs']\n",
    "headline_tokens = [['i', 'like', 'cats'], ['one', 'two', 'three'], ['i', 'like', 'dogs']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like cats\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-1a14963395ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_headlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "for token, headline in zip(headline_tokens, news_headlines):\n",
    "    print(headline)\n",
    "    print(\"    \" + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
